Greatâ€”letâ€™s slot these into Sprint 2 and jump straight into **2.6: Vault Indexing**.

---

### ğŸ“… Updated Sprint 2 Backlog

| #    | Task                                        | Owner | Status |
|------|---------------------------------------------|:-----:|:------:|
|2.6   | **Index Vault Documents**                   | Me    | **In Progress**  |
|      | â€“ Build a vector index of key PDF sections  |       |        |
|      | â€“ Store embeddings in a JSON store          |       |        |
|2.7   | Retrieval API                               | Me    | To Do  |
|2.8   | Draft API Enhancement                       | Me    | To Do  |
|2.9   | UI: Context Preview Panel                   | Me    | To Do  |
|2.10  | Spinner & Fallback                          | Me    | To Do  |

---

### ğŸ”¨ 2.6 Vault Indexing: Implementation

Weâ€™ll create a new file at `server/brain/vaultIndexer.js` that:

1. Reads your Vault metadata (list of PDFs).  
2. Runs each PDF through **pdf-parse** to extract text.  
3. Splits text into ~1,000-char chunks.  
4. Feeds each chunk to OpenAIâ€™s Embeddings API.  
5. Dumps an array of `{ docId, chunkId, text, embedding }` to `server/brain/embeddings.json`.

---

#### ğŸ“‹ Canvas #1: Create the directory and indexer file

```bash
# In your Replit shell:
mkdir -p server/brain
```

```bash
# Create vaultIndexer.js
touch server/brain/vaultIndexer.js
```

---

#### ğŸ“„ Canvas #2: `server/brain/vaultIndexer.js`

```javascript
import fs from "fs";
import path from "path";
import pdfParse from "pdf-parse";
import { Configuration, OpenAIApi } from "openai";

// 1) Configure OpenAI
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

// 2) Paths
const METADATA_FILE = path.join(__dirname, "../vault/metadata.json");
const OUTPUT_FILE   = path.join(__dirname, "embeddings.json");

// 3) Helpers
async function chunkText(text, maxLen = 1000) {
  const chunks = [];
  let start = 0;
  while (start < text.length) {
    let end = Math.min(start + maxLen, text.length);
    // try to cut at paragraph break
    const slice = text.slice(start, end);
    const lastBreak = slice.lastIndexOf("\n\n");
    if (lastBreak > 200) end = start + lastBreak;
    chunks.push(text.slice(start, end).trim());
    start = end;
  }
  return chunks;
}

// 4) Main indexing
async function buildIndex() {
  console.log("â³ Loading metadata...");
  const metadata = JSON.parse(fs.readFileSync(METADATA_FILE, "utf8"));
  const embeddings = [];

  for (const doc of metadata.documents) {
    const filePath = path.join(__dirname, "../vault", doc.path);
    console.log(`ğŸ“„ Parsing PDF: ${doc.id} â†’ ${doc.path}`);
    const dataBuffer = fs.readFileSync(filePath);
    const { text } = await pdfParse(dataBuffer);

    const chunks = await chunkText(text);
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      process.stdout.write(`  â€¢ embedding chunk ${i+1}/${chunks.length}â€¦\r`);
      const resp = await openai.createEmbedding({
        model: "text-embedding-ada-002",
        input: chunk,
      });
      embeddings.push({
        docId: doc.id,
        chunkId: i,
        text: chunk,
        embedding: resp.data.data[0].embedding,
      });
    }
    console.log(`\nâœ… Indexed ${chunks.length} chunks for ${doc.id}`);
  }

  fs.writeFileSync(OUTPUT_FILE, JSON.stringify(embeddings, null, 2));
  console.log(`ğŸ‰ Done! Wrote ${embeddings.length} embeddings to ${OUTPUT_FILE}`);
}

buildIndex().catch(err => {
  console.error("âŒ Indexing failed:", err);
  process.exit(1);
});
```

---

#### ğŸš€ Next Steps

1. **Install dependencies**  
   ```bash
   npm install pdf-parse openai
   ```
2. **Run the indexer**  
   ```bash
   node server/brain/vaultIndexer.js
   ```
3. Once `embeddings.json` is ready, weâ€™ll build **2.7: Retrieval API**.

Let me know when indexing completes (or if you hit any snags), and weâ€™ll wire up the retrieval endpoint next!